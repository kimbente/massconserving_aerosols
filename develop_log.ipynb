{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Description : Train neural network model to predict one time step of M7\n",
    "Options:\n",
    "\n",
    "  --signs=<need_extra_signs_for_log_mass>\n",
    "  --classification=<train_classification_net>\n",
    "  --scale=<scaler>\n",
    "  --model=<model_version>\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from utils import standard_transform_x, standard_transform_y, get_model, train_model, create_report, calculate_stats, log_full_norm_transform_x, log_tend_norm_transform_y, create_dataloader, create_test_dataloader\n",
    "# from models import Softmax_model\n",
    "from utils import add_nn_arguments_jupyter\n",
    "import torch.nn as nn \n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# KB add for active development in models or utils\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define full path \n",
    "path_to_data = \"/home/kim/data/aerosols/aerosol_emulation_data/\"\n",
    "\n",
    "X_test = np.load(path_to_data + 'X_test.npy')\n",
    "y_test = np.load(path_to_data + 'y_test.npy')\n",
    "\n",
    "X_train = np.load(path_to_data + 'X_train.npy')\n",
    "y_train = np.load(path_to_data + 'y_train.npy')\n",
    "\n",
    "X_valid = np.load(path_to_data + 'X_val.npy')\n",
    "y_valid = np.load(path_to_data + 'y_val.npy')\n",
    "\n",
    "# Select the correct 24 columns\n",
    "X_test_24 = X_test[:, 8:]\n",
    "X_train_24 = X_train[:, 8:] \n",
    "\n",
    "y_test_24 = y_test[:, :24]\n",
    "y_train_24 = y_train[:, :24]\n",
    "\n",
    "y_valid_24 = y_valid[:, :24]\n",
    "X_valid_24 = X_valid[:, 8:]\n",
    "\n",
    "# How much has it changes between x (at t = 0)  and y (at t = 1)\n",
    "y_delta_train_24 = y_train_24 - X_train_24\n",
    "y_delta_test_24 = y_test_24 - X_test_24\n",
    "y_delta_valid_24 = y_valid_24 - X_valid_24\n",
    "\n",
    "# Define column indices for each of the components (24 column version)\n",
    "so4_indices = [0, 1, 2, 3, 4]\n",
    "bc_indices = [5, 6, 7, 8]\n",
    "oc_indices = [9, 10, 11, 12]\n",
    "du_indices = [13, 14, 15, 16]\n",
    "\n",
    "# Define aerosol species and their corresponding indices\n",
    "species_indices = {\n",
    "    'so4': so4_indices,\n",
    "    'bc': bc_indices,\n",
    "    'oc': oc_indices,\n",
    "    'du': du_indices\n",
    "}\n",
    "\n",
    "# What are these indices?!\n",
    "extra_indices = [17, 18, 19, 20, 21, 22, 23] \n",
    "\n",
    "# Define aerosol species and their corresponding indices\n",
    "\n",
    "### ARGS ###\n",
    "args = add_nn_arguments_jupyter()\n",
    "# Overwrite the model name, keep everything else the same\n",
    "# Have one model for now as each input dim can be different\n",
    "args.model = 'transition_model'\n",
    "# args.model_id = 'transition_' + species # save different models\n",
    "# Run for only 3 epochs for proof of concept\n",
    "# Took around 2 mins per epoch\n",
    "args.epochs = 3 \n",
    "### DIFFERENT DIMS\n",
    "# Takes a minute\n",
    "# stats = calculate_stats(X_train, (y_train - X_train), X_test, (y_test - X_test), args)\n",
    "# y's can be delata and 24, X is raw\n",
    "stats = calculate_stats(X_train, y_delta_train_24, X_test, y_delta_test_24, args)\n",
    "\n",
    "# Look at stats\n",
    "np.set_printoptions(precision = 4, suppress = True, formatter = {'all': lambda x: f'{x:.4f}'})\n",
    "# stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogSoftmax_model(nn.Module):\n",
    "    def __init__(self, in_features, out_features, width, depth = 2):\n",
    "        super(LogSoftmax_model, self).__init__()\n",
    "        self.fc_in = nn.Linear(in_features = in_features, out_features = width)\n",
    "        # Create the hidden layers\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        for i in range(depth - 1):\n",
    "            self.hidden_layers.append(nn.ReLU())\n",
    "            self.hidden_layers.append(nn.Linear(in_features = width, out_features = width))\n",
    "            self.hidden_layers.append(nn.ReLU())\n",
    "        # Output layer (fc: fully connected)\n",
    "        self.fc_out = nn.Linear(in_features = width, out_features = out_features + 1)\n",
    "        # ADD softmax layer: same as probabilities per class (classification)\n",
    "        self.softmax = nn.Softmax(dim = -1)  # Apply softmax along the output dimension\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x_relative = \n",
    "        x_log = torch.log(x + 1e-8)  # Apply log transformation to the input\n",
    "        # Pass through the input layer (fully connected)\n",
    "        out = self.fc_in(x_log)\n",
    "        # Pass through hidden layers\n",
    "        for layer in self.hidden_layers:\n",
    "            out = layer(out)\n",
    "        # Final output layer\n",
    "        out = self.fc_out(out)\n",
    "\n",
    "        # Split it up\n",
    "        softmax_input = out[:, :-1]\n",
    "        scalar = out[:, -1]\n",
    "\n",
    "        # Apply softmax to the final output for classification\n",
    "        # calculate in double precision\n",
    "        softmax_out = self.softmax(softmax_input).double()\n",
    "\n",
    "        # Make softmax zero_sum\n",
    "        zero_sum_output = softmax_out - softmax_out.mean(dim = -1, keepdim = True)\n",
    "\n",
    "        scaled_zero_sum_output = zero_sum_output * scalar.unsqueeze(1)\n",
    "\n",
    "        # Avoid division by zero by ensuring denominator is never zero\n",
    "        denominator = scaled_zero_sum_output.clone()\n",
    "        denominator = torch.where(denominator == 0, torch.tensor(1e-10).to(denominator.device), denominator)\n",
    "\n",
    "        ### Ensure non-negativivity constraint ###\n",
    "        safe_beta = torch.where((\n",
    "            (scaled_zero_sum_output + x) < 0), # In the case of negative values\n",
    "            (- x / denominator), # scalar candidates, maybe add noise?! works per row\n",
    "            torch.tensor(float('inf')) # infinity if no violation (so it doesn't get selected)\n",
    "            ).min(dim = 1)[0].unsqueeze(-1) # select the minimum value over columns (for each row)\n",
    "            # minimum by which we have to scale it backk. 0 in worst case\n",
    "        # unsqueeze to make torch.Size([n_batch, 1])\n",
    "\n",
    "        # In case of no violation (safe_beta == inf), set beta to 1 - no change occurs\n",
    "        # In case of violation, set beta to the minimum value (zero in worst case)\n",
    "        # row-wise i.e. batch-wise min selection\n",
    "        beta = torch.min(torch.ones(safe_beta.shape), safe_beta)\n",
    "\n",
    "        safe_out = beta * zero_sum_output\n",
    "\n",
    "        return safe_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_so4_subset = X_train_24[np.ix_([-4, 0, 1000, 20000, 21000, 400000, -1], so4_indices)]\n",
    "y_train_so4_subset = y_train_24[np.ix_([-4, 0, 1000, 20000, 21000, 400000, -1], so4_indices)]\n",
    "y_delta_train_so4_subset = y_delta_train_24[np.ix_([-4, 0, 1000, 20000, 21000, 400000, -1], so4_indices)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000e+00, 1.3182e+04, 2.6606e+08, 6.9101e+06, 1.3711e+05],\n",
       "        [2.4033e+01, 1.3761e+03, 3.6511e+05, 5.0815e+05, 1.5746e+02],\n",
       "        [3.4661e+01, 1.5090e+03, 3.0621e+05, 4.0179e+05, 6.1046e+01],\n",
       "        [2.2522e+01, 2.6219e+03, 1.2649e+06, 2.7109e+06, 1.1954e+03],\n",
       "        [6.1790e+01, 9.6141e+02, 1.9868e+06, 7.6254e+06, 6.5636e+03],\n",
       "        [2.7293e+06, 2.4266e+03, 2.2450e+08, 7.1790e+08, 6.0305e+07],\n",
       "        [0.0000e+00, 1.3179e+04, 2.6545e+08, 7.3312e+06, 1.5054e+05]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0995, 0.0528, 0.3892, 0.4178, 0.0407],\n",
      "        [0.0382, 0.1223, 0.4379, 0.3292, 0.0725],\n",
      "        [0.0389, 0.1317, 0.4294, 0.3286, 0.0714],\n",
      "        [0.0315, 0.1037, 0.4659, 0.3349, 0.0640],\n",
      "        [0.0281, 0.1026, 0.4739, 0.3285, 0.0670],\n",
      "        [0.0063, 0.0606, 0.5250, 0.3565, 0.0516],\n",
      "        [0.0991, 0.0527, 0.3896, 0.4179, 0.0407]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.6785],\n",
      "        [0.9608],\n",
      "        [0.9130],\n",
      "        [1.1076],\n",
      "        [1.2154],\n",
      "        [2.0176],\n",
      "        [0.6809]], grad_fn=<UnsqueezeBackward0>)\n",
      "scaled\n",
      "tensor([[-0.0682, -0.0999,  0.1284,  0.1478, -0.1081],\n",
      "        [-0.1554, -0.0747,  0.2285,  0.1241, -0.1225],\n",
      "        [-0.1471, -0.0624,  0.2095,  0.1174, -0.1174],\n",
      "        [-0.1867, -0.1067,  0.2946,  0.1495, -0.1507],\n",
      "        [-0.2089, -0.1184,  0.3329,  0.1561, -0.1617],\n",
      "        [-0.3909, -0.2812,  0.6556,  0.3157, -0.2993],\n",
      "        [-0.0687, -0.1003,  0.1291,  0.1484, -0.1085]], grad_fn=<MulBackward0>)\n",
      "safe_beta\n",
      "torch.Size([7, 1])\n",
      "tensor([[0.],\n",
      "        [inf],\n",
      "        [inf],\n",
      "        [inf],\n",
      "        [inf],\n",
      "        [inf],\n",
      "        [0.]], grad_fn=<UnsqueezeBackward0>)\n",
      "beta\n",
      "torch.Size([7, 1])\n",
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.]], grad_fn=<MinimumBackward0>)\n",
      "tensor([[-0.0000, -0.0000,  0.0000,  0.0000, -0.0000],\n",
      "        [-0.1618, -0.0777,  0.2379,  0.1292, -0.1275],\n",
      "        [-0.1611, -0.0683,  0.2294,  0.1286, -0.1286],\n",
      "        [-0.1685, -0.0963,  0.2659,  0.1349, -0.1360],\n",
      "        [-0.1719, -0.0974,  0.2739,  0.1285, -0.1330],\n",
      "        [-0.1937, -0.1394,  0.3250,  0.1565, -0.1484],\n",
      "        [-0.0000, -0.0000,  0.0000,  0.0000, -0.0000]], grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0000, -0.0000,  0.0000,  0.0000, -0.0000],\n",
       "        [-0.1618, -0.0777,  0.2379,  0.1292, -0.1275],\n",
       "        [-0.1611, -0.0683,  0.2294,  0.1286, -0.1286],\n",
       "        [-0.1685, -0.0963,  0.2659,  0.1349, -0.1360],\n",
       "        [-0.1719, -0.0974,  0.2739,  0.1285, -0.1330],\n",
       "        [-0.1937, -0.1394,  0.3250,  0.1565, -0.1484],\n",
       "        [-0.0000, -0.0000,  0.0000,  0.0000, -0.0000]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogSoftmax_model(\n",
    "    in_features = x_train_so4_subset.shape[-1], \n",
    "    out_features = y_delta_train_so4_subset.shape[-1], \n",
    "    width = 128, depth = 2)\n",
    "\n",
    "input = torch.tensor(x_train_so4_subset, dtype = torch.float64)\n",
    "# force issues: input = input - input *0.9999\n",
    "out = model(input)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# elif args.scale == 'log':\n",
    "#            y_test = log_tend_norm_transform_y_inv(stats, y_test)\n",
    "#            X_test = log_full_norm_transform_x_inv(stats, X_test)\n",
    "\n",
    "def log_transform(x):\n",
    "    return np.log(np.abs(x)+1e-8) # absolute? What about signs?\n",
    "\n",
    "def exp_transform(x):\n",
    "    return np.exp(x)-1e-8\n",
    "\n",
    "# Tranforms applied in paper\n",
    "def log_tend_norm_transform_y(stats, x):\n",
    "    x = log_transform(x)\n",
    "    x = (x - stats['y_log_eps_mean'])/stats['y_log_eps_std']\n",
    "    return x\n",
    "\n",
    "def log_full_norm_transform_x(stats, x):\n",
    "    x = log_transform(x)\n",
    "    x = (x - stats['X_log_eps_mean'])/stats['X_log_eps_std']\n",
    "    return x\n",
    "\n",
    "# INVERSE TRANSFORM PRED\n",
    "\n",
    "# elif args.scale == 'log':\n",
    "#        pred = log_tend_norm_transform_y_inv(stats, pred)\n",
    "\n",
    "def log_tend_norm_transform_y_inv(stats, x):    \n",
    "    x = x*stats['y_log_eps_std']+stats['y_log_eps_mean']\n",
    "    x = exp_transform(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input\n",
      "tensor([[    0.000000,     1.318359, 26608.000000,   691.000000,    13.718750],\n",
      "        [    0.002403,     0.137695,    36.531250,    50.812500,     0.015747],\n",
      "        [    0.003468,     0.150879,    30.625000,    40.187500,     0.006107],\n",
      "        [    0.002253,     0.262207,   126.500000,   271.250000,     0.119507],\n",
      "        [    0.006180,     0.096130,   198.750000,   762.500000,     0.656250],\n",
      "        [  273.000000,     0.242676, 22448.000000, 71808.000000,  6032.000000],\n",
      "        [    0.000000,     1.318359, 26544.000000,   733.000000,    15.062500]])\n",
      "Output (delta)\n",
      "tensor([[-0., 0., -0., -0., 0.],\n",
      "        [-0., 0., -0., -0., 0.],\n",
      "        [-0., 0., 0., 0., 0.],\n",
      "        [-0., 0., -0., -0., 0.],\n",
      "        [-0., 0., -0., -0., 0.],\n",
      "        [0., 0., -0., -0., 0.],\n",
      "        [-0., 0., -0., -0., 0.]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "Input + Output (delta)\n",
      "We have a violation if this is negativ.\n",
      "tensor([[    0.000000,     1.318359, 26608.000000,   691.000000,    13.718750],\n",
      "        [    0.002403,     0.137695,    36.531250,    50.812500,     0.015747],\n",
      "        [    0.003468,     0.150879,    30.625000,    40.187500,     0.006107],\n",
      "        [    0.002253,     0.262207,   126.500000,   271.250000,     0.119507],\n",
      "        [    0.006180,     0.096130,   198.750000,   762.500000,     0.656250],\n",
      "        [  273.000000,     0.242676, 22448.000000, 71808.000000,  6032.000000],\n",
      "        [    0.000000,     1.318359, 26544.000000,   733.000000,    15.062500]],\n",
      "       dtype=torch.float64, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.set_printoptions(precision = 6, sci_mode = False)\n",
    "model = LogSoftmax_model(\n",
    "    in_features = x_train_so4_subset.shape[-1], \n",
    "    out_features = y_delta_train_so4_subset.shape[-1], \n",
    "    width = 128, depth = 2)\n",
    "\n",
    "inp = torch.tensor(x_train_so4_subset, dtype = torch.float32)\n",
    "inp = inp - inp*0.9999\n",
    "print(\"Input\")\n",
    "print(inp)\n",
    "\n",
    "out = model(inp)\n",
    "print(\"Output (delta)\")\n",
    "print(out)\n",
    "\n",
    "print(\"Input + Output (delta)\")\n",
    "print(\"We have a violation if this is negativ.\")\n",
    "# Violation is this is negative\n",
    "print(inp + out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0.0000,        inf,        inf,        inf,        inf],\n",
       "        [    0.0428,        inf,        inf,        inf,        inf],\n",
       "        [    0.0628,        inf,        inf,        inf,        inf],\n",
       "        [    0.0283,        inf,        inf,        inf,        inf],\n",
       "        [    0.0751,        inf,        inf,        inf,        inf],\n",
       "        [       inf,        inf,        inf,        inf,        inf],\n",
       "        [    0.0000,        inf,        inf,        inf,        inf]],\n",
       "       dtype=torch.float64, grad_fn=<WhereBackward0>)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.where(((out + inp) < 0), (-inp / out) + 1e-5, torch.tensor(float('inf')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[     0.0000,    -12.1310, -283228.6969,   4177.9132,   -135.3324],\n",
       "        [     0.0428,     -1.5328,  -1377.5941,    562.5725,     -0.5226],\n",
       "        [     0.0628,     -1.5061,  -1116.3906,    467.0323,     -0.4467],\n",
       "        [     0.0283,     -3.0542,  -3340.6744,   2395.6851,     -1.7310],\n",
       "        [     0.0751,     -1.3428,  -4804.7660,   6363.0303,     -7.3601],\n",
       "        [  3655.2058,     -1.4863, -1276366.1838, 458349.6959, -119457.9955],\n",
       "        [     0.0000,    -12.2776, -279312.6114,   4428.3950,   -147.6224]],\n",
       "       dtype=torch.float64, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Input is always non-negative so \n",
    "# Beta must be larger than this ratio\n",
    "-inp/out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([    0.0000,     0.0428,     0.0628,     0.0283,     0.0751,  3655.2058,\n",
      "            0.0000], dtype=torch.float64, grad_fn=<MinBackward0>)\n",
      "tensor([0.], grad_fn=<MinimumBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[    0.0000,     1.3184, 26608.0000,   691.0000,    13.7188],\n",
       "        [    0.0024,     0.1377,    36.5312,    50.8125,     0.0157],\n",
       "        [    0.0035,     0.1509,    30.6250,    40.1875,     0.0061],\n",
       "        [    0.0023,     0.2622,   126.5000,   271.2500,     0.1195],\n",
       "        [    0.0062,     0.0961,   198.7500,   762.5000,     0.6562],\n",
       "        [  273.0000,     0.2427, 22448.0000, 71808.0000,  6032.0000],\n",
       "        [    0.0000,     1.3184, 26544.0000,   733.0000,    15.0625]],\n",
       "       dtype=torch.float64, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "safe_beta = torch.where(out < 0, -inp / out, torch.tensor(float('inf')).to(out.device)).min(dim = 1)[0]\n",
    "print(safe_beta)\n",
    "beta = torch.min(torch.ones(1), torch.min(safe_beta))\n",
    "print(beta)\n",
    "beta.unsqueeze(-1) * out + inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avoid division by zero; only consider elements where out ≠ 0\n",
    "safe_beta = torch.where(out != 0, -inp / out, torch.tensor(float('inf')).to(out.device))\n",
    "\n",
    "# Beta should be the smallest positive value ensuring non-negativity\n",
    "beta = torch.min(torch.ones(1), torch.min(safe_beta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[     0.1581,     -5.0879,    340.5997,   -942.7436,     -0.3795],\n",
       "        [     0.1581,     -5.0899,    340.5916,   -943.3430,     -0.3794],\n",
       "        [     0.1582,     -5.0914,    340.6124,   -943.4422,     -0.3793],\n",
       "        [     0.1582,     -5.0914,    340.6124,   -943.4422,     -0.3793],\n",
       "        [     0.1583,     -5.0925,    340.6102,   -943.4239,     -0.3796],\n",
       "        [     0.1583,     -5.0925,    340.6102,   -943.4239,     -0.3796],\n",
       "        [     0.1583,     -5.0925,    340.6101,   -943.4243,     -0.3796]],\n",
       "       dtype=torch.float64, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "- inp / out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.2800e-02,  1.6476e-01,  3.6424e+01,  5.0866e+01,  5.7245e-02],\n",
      "        [-1.2797e-02,  1.6475e-01,  3.6424e+01,  5.0898e+01,  5.7256e-02],\n",
      "        [-1.2795e-02,  1.6474e-01,  3.6424e+01,  5.0898e+01,  5.7262e-02],\n",
      "        [-1.2795e-02,  1.6474e-01,  3.6424e+01,  5.0898e+01,  5.7262e-02],\n",
      "        [-1.2793e-02,  1.6473e-01,  3.6424e+01,  5.0898e+01,  5.7281e-02],\n",
      "        [-1.2793e-02,  1.6473e-01,  3.6424e+01,  5.0898e+01,  5.7281e-02],\n",
      "        [-1.2793e-02,  1.6473e-01,  3.6424e+01,  5.0898e+01,  5.7281e-02]],\n",
      "       dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0128, -0.0128, -0.0128, -0.0128, -0.0128, -0.0128, -0.0128],\n",
      "       dtype=torch.float64, grad_fn=<MinBackward0>)\n",
      "tensor([[ 0.0000,  0.1648, 36.4240, 50.8664,  0.0572],\n",
      "        [ 0.0000,  0.1647, 36.4240, 50.8976,  0.0573],\n",
      "        [ 0.0000,  0.1647, 36.4240, 50.8976,  0.0573],\n",
      "        [ 0.0000,  0.1647, 36.4240, 50.8976,  0.0573],\n",
      "        [ 0.0000,  0.1647, 36.4240, 50.8976,  0.0573],\n",
      "        [ 0.0000,  0.1647, 36.4240, 50.8976,  0.0573],\n",
      "        [ 0.0000,  0.1647, 36.4240, 50.8976,  0.0573]], dtype=torch.float64,\n",
      "       grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# first row would be neg\n",
    "print(out + inp)\n",
    "# Select the minimum per row, because this is what we need to set to zero\n",
    "print(torch.min(out + inp, axis = -1)[0])\n",
    "violation = torch.min(out + inp, axis = -1)[0]\n",
    "violation_tiled = torch.tile(violation.unsqueeze(-1), (1, inp.shape[-1]))\n",
    "violation_tiled - inp\n",
    "\n",
    "print(torch.relu(out + inp))\n",
    "# print(torch.nn.functional.softplus(out + inp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[     1.0000,     -5.5609,    340.7190,   -942.9811,     -0.6879],\n",
       "        [     1.0000,     -5.5630,    340.7109,   -943.5805,     -0.6877],\n",
       "        [     1.0000,     -5.5645,    340.7317,   -943.6796,     -0.6875],\n",
       "        [     1.0000,     -5.5645,    340.7317,   -943.6796,     -0.6875],\n",
       "        [     1.0000,     -5.5657,    340.7294,   -943.6613,     -0.6878],\n",
       "        [     1.0000,     -5.5657,    340.7294,   -943.6613,     -0.6878],\n",
       "        [     1.0000,     -5.5657,    340.7294,   -943.6617,     -0.6878]],\n",
       "       dtype=torch.float64, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_printoptions(precision = 4, sci_mode = False)\n",
    "(violation_tiled - inp) / out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.5226e+00, -9.8981e+01,  1.4223e+05,  1.9712e+05,  4.5718e+01],\n",
       "        [ 4.5232e+00, -9.8994e+01,  1.4224e+05,  1.9715e+05,  4.5727e+01],\n",
       "        [ 4.5237e+00, -9.9007e+01,  1.4224e+05,  1.9718e+05,  4.5736e+01],\n",
       "        [ 4.5243e+00, -9.9019e+01,  1.4224e+05,  1.9721e+05,  4.5745e+01],\n",
       "        [ 4.5248e+00, -9.9031e+01,  1.4225e+05,  1.9724e+05,  4.5752e+01],\n",
       "        [ 4.5252e+00, -9.9042e+01,  1.4225e+05,  1.9727e+05,  4.5760e+01],\n",
       "        [ 4.5257e+00, -9.9053e+01,  1.4225e+05,  1.9730e+05,  4.5767e+01]],\n",
       "       dtype=torch.float64, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "- inp / (out + 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  595.9630, 14112.4749,     0.0000,     0.0000,  1817.9433],\n",
       "        [  596.0211, 14114.2689,     0.0000,     0.0000,  1818.1510],\n",
       "        [  596.0771, 14116.0047,     0.0000,     0.0000,  1818.3509],\n",
       "        [  596.1312, 14117.6609,     0.0000,     0.0000,  1818.5434],\n",
       "        [  596.1830, 14119.2519,     0.0000,     0.0000,  1818.7272],\n",
       "        [  596.2323, 14120.7728,     0.0000,     0.0000,  1818.9036],\n",
       "        [  596.2796, 14122.2215,     0.0000,     0.0000,  1819.0703]],\n",
       "       dtype=torch.float64, grad_fn=<MaximumBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This sum must be non-negative\n",
    "(out + inp)\n",
    "torch.relu()\n",
    "\n",
    "safeguard_scalar = torch.max(- inp / (out + 1e-8), torch.zeros_like(inp)) \n",
    "safeguard_scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.4033e+01, 1.3761e+03, 3.6511e+05, 5.0815e+05, 1.5746e+02],\n",
       "        [2.4036e+01, 1.3763e+03, 3.6515e+05, 5.0822e+05, 1.5748e+02],\n",
       "        [2.4039e+01, 1.3765e+03, 3.6520e+05, 5.0828e+05, 1.5750e+02],\n",
       "        [2.4042e+01, 1.3766e+03, 3.6524e+05, 5.0834e+05, 1.5752e+02],\n",
       "        [2.4044e+01, 1.3768e+03, 3.6528e+05, 5.0840e+05, 1.5754e+02],\n",
       "        [2.4047e+01, 1.3769e+03, 3.6532e+05, 5.0845e+05, 1.5755e+02],\n",
       "        [2.4049e+01, 1.3771e+03, 3.6536e+05, 5.0850e+05, 1.5757e+02]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.tensor(x_train_so4_subset, dtype = torch.float32))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aero",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
